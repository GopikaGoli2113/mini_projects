# -*- coding: utf-8 -*-
"""laptop_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zq9ztfPYh5Q1LFskVSGC5qNNCEbStKMs

# **1. Project Setup**

Install necessary libraries:
"""

!pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm

"""# **2. Data Exploration & Understanding**

**Tasks:**

‚úÖ Check for missing values and duplicate records.

‚úÖ Explore the distribution of numerical features using histograms.

‚úÖ Analyze categorical variables using count plots.

‚úÖ Identify relationships between variables using correlation heatmaps and pair plots.

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/laptop.csv")

# Check data types and missing values
print(df.info())
print(df.isnull().sum())

# Summary statistics
print(df.describe())

# Visualize price distribution
plt.figure(figsize=(10, 5))
sns.histplot(df['Price'], bins=30, kde=True)
plt.title("Laptop Price Distribution")
plt.show()

"""# **Step 3: Data Preprocessing**

**üîπ Tasks:**

**‚úÖ Identify Numeric and Categorical Columns**

*  Identify columns with numerical data (int, float).
*  Identify columns with categorical data (object, string).

**‚úÖ Handle Missing Values**


*  **Numeric Columns:** Fill missing values with the median to avoid extreme outliers affecting the data.
*  **Categorical Columns:** Fill missing values with the mode (most frequent category).

**‚úÖ Encode Categorical Features**

*  Use Label Encoding to convert categorical values into numerical representations, allowing machine learning models to process them.

**‚úÖ Scale Numerical Features**


*  Use StandardScaler to normalize numeric data, ensuring features have a mean of 0 and a standard deviation of 1 for better model performance.

"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load dataset (if not already loaded)
df = pd.read_csv("laptop.csv")

# Standardize column names to avoid case issues
df.columns = df.columns.str.strip().str.lower()  # Convert all column names to lowercase

# Identify numeric and categorical columns
numeric_cols = df.select_dtypes(include=['number']).columns
categorical_cols = df.select_dtypes(exclude=['number']).columns

# Fill missing values
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())  # Numeric: Median
df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])  # Categorical: Mode

# Verify missing values are handled
print("Missing values after preprocessing:\n", df.isnull().sum())

# Encode categorical features using Label Encoding
encoder = LabelEncoder()
for col in categorical_cols:
    df[col] = encoder.fit_transform(df[col])

# Scale numerical features using StandardScaler
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Display the first few rows of the processed dataset
print("\nProcessed Data Sample:")
print(df.head())

"""### **Step 4: Feature Extraction & Transformation**
üîπ **Tasks:**

‚úÖ **Fix Column Names for Consistency**  
- Converts all column names to **lowercase** to avoid case sensitivity issues.

‚úÖ **Drop Unnecessary Columns**  
- Removes index-related columns that contain **‚ÄòUnnamed‚Äô**, which are not useful for analysis.

‚úÖ **Convert RAM & Weight to Numeric Values**  
- **Removes unit labels ("GB", "kg")** from RAM and Weight columns.  
- **Converts RAM to integer** and Weight to float type for numerical processing.  
- **Handles missing values** by filling with the **median**.

‚úÖ **Extract Screen Resolution Details**  
- Splits the `screenresolution` column into two **numerical features**:  
  - `resolution_width` (e.g., 1920)  
  - `resolution_height` (e.g., 1080)  

‚úÖ **Create Aspect Ratio Feature**  
- Computes the **aspect ratio** using the formula:  
  \[
  \text{Aspect Ratio} = \frac{\text{Resolution Width}}{\text{Resolution Height}}
  \]
- This helps capture screen proportions (e.g., **16:9, 4:3**).

‚úÖ **Extract Storage Type (SSD & HDD)**  
- Creates **two binary features** from the `memory` column:  
  - `ssd = 1` if laptop has an **SSD**, otherwise `0`.  
  - `hdd = 1` if laptop has an **HDD**, otherwise `0`.  

‚úÖ **Extract CPU & GPU Brands**  
- Extracts the **brand name** from the `cpu` and `gpu` columns:  
  - Example: `"Intel Core i7"` ‚Üí `cpu_brand = Intel`  
  - Example: `"NVIDIA GTX 1650"` ‚Üí `gpu_brand = NVIDIA`  

‚úÖ **Encode Categorical Variables**  
- Uses **Label Encoding** to convert categorical columns into **numeric values** for machine learning.  
  - Columns encoded: `'company', 'typename', 'cpu_brand', 'gpu_brand', 'opsys'`.  

‚úÖ **Drop Unnecessary Columns After Feature Extraction**  
- Removes columns that are no longer needed:  
  - `'screenresolution', 'memory', 'cpu', 'gpu'`.  

This feature extraction step **enhances the dataset** by creating structured and meaningful numerical features, making it more suitable for machine learning models. üöÄ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv("/content/laptop.csv")

# Fix column names (lowercase for consistency)
df.columns = df.columns.str.lower()

# Drop unnecessary index columns
df = df.loc[:, ~df.columns.str.contains('unnamed')]

# Handle missing values in RAM & Weight before type conversion
df['ram'] = df['ram'].astype(str).str.replace("GB", "", regex=True)
df['ram'] = pd.to_numeric(df['ram'], errors='coerce')  # Convert to numeric
df['ram'] = df['ram'].fillna(df['ram'].median()).astype(int)  # Fill NaNs and convert to int

df['weight'] = df['weight'].astype(str).str.replace("kg", "", regex=True)
df['weight'] = pd.to_numeric(df['weight'], errors='coerce')  # Convert to float
df['weight'] = df['weight'].fillna(df['weight'].median())  # Fill NaNs

# Extract screen resolution details
df[['resolution_width', 'resolution_height']] = df['screenresolution'].str.extract(r'(\d+)x(\d+)').astype(float)

# Feature: Aspect Ratio
df['aspect_ratio'] = df['resolution_width'] / df['resolution_height']

# Handle missing values in the 'memory' column
df['memory'] = df['memory'].astype(str).fillna('Unknown')

# Feature: SSD vs HDD (Extract from 'Memory' column safely)
df['ssd'] = df['memory'].apply(lambda x: 1 if isinstance(x, str) and 'SSD' in x else 0)
df['hdd'] = df['memory'].apply(lambda x: 1 if isinstance(x, str) and 'HDD' in x else 0)

# Extract CPU and GPU brand
df['cpu_brand'] = df['cpu'].astype(str).apply(lambda x: x.split()[0] if isinstance(x, str) else 'Unknown')
df['gpu_brand'] = df['gpu'].astype(str).apply(lambda x: x.split()[0] if isinstance(x, str) else 'Unknown')

# Encode categorical variables
label_cols = ['company', 'typename', 'cpu_brand', 'gpu_brand', 'opsys']
encoder = LabelEncoder()
for col in label_cols:
    df[col] = encoder.fit_transform(df[col].astype(str))  # Convert to string before encoding

# Drop original columns that are no longer needed after feature extraction
df.drop(columns=['screenresolution', 'memory', 'cpu', 'gpu'], inplace=True)

# Display processed dataset
print("\nFeature-Engineered Data Sample:")
print(df.head())

"""# ***5: Model Development***
Model development involves training machine learning models on preprocessed data and evaluating their performance.

**Tasks:**

**‚úÖ Split Data into Training & Testing Sets**

*   Separate features (X) and target (y) (Price).
*   Split data into training (80%) and testing (20%) sets using train_test_split().

**‚úÖ Train Multiple Machine Learning Models**

*   Train different models for price prediction, such as: Linear Regression (Baseline model)
*   Random Forest Regressor (Ensemble method)
Gradient Boosting Regressor (Boosting method for better performance)

**‚úÖ Evaluate Performance Using Metrics**

**Mean Absolute Error (MAE):** Measures average absolute difference between actual and predicted prices.

**Root Mean Squared Error (RMSE):** Penalizes larger errors more than MAE.

**R¬≤ Score:** Measures how well the model explains the variance in laptop prices (1 = perfect fit, 0 = no relationship).
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load dataset
df = pd.read_csv("/content/laptop.csv")

# Drop unnecessary columns
df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], errors='ignore', inplace=True)

# Convert column names to lowercase
df.columns = df.columns.str.lower()

# Handle missing values
df.fillna(df.median(numeric_only=True), inplace=True)  # Fill numeric NaNs with median

# Convert 'ram' and 'weight' to numeric
df['ram'] = df['ram'].astype(str).str.replace("GB", "").astype(float)
# Convert 'weight' column to string and clean invalid values
df['weight'] = df['weight'].astype(str).str.replace("kg", "")

# Replace invalid values like '?' with NaN
df['weight'] = pd.to_numeric(df['weight'], errors='coerce')  # Convert to float, setting invalid values as NaN

# Fill missing or invalid values with median weight
df['weight'].fillna(df['weight'].median(), inplace=True)


# Handle 'memory' column to extract SSD and HDD separately
df['ssd'] = df['memory'].apply(lambda x: 1 if isinstance(x, str) and 'SSD' in x else 0)
df['hdd'] = df['memory'].apply(lambda x: 1 if isinstance(x, str) and 'HDD' in x else 0)
df.drop(columns=['memory'], inplace=True)  # Drop original 'memory' column

# One-Hot Encoding for categorical variables
categorical_cols = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Define features and target variable
X = df.drop(columns=['price'], errors='ignore')  # Ensure 'price' column exists
y = df['price']

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared Score (R¬≤): {r2}")

"""# **6. Hyperparameter Tuning**

Use GridSearchCV or RandomizedSearchCV to optimize model parameters.
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

print(grid_search.best_params_)

"""# **7. Model Interpretation**

Use SHAP or feature importance analysis to explain predictions.
"""

import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test)

"""# **8: Real-Time Predictions**

**üîπ Tasks:**

**‚úÖ Prepare Input Data for Prediction**

Create a new DataFrame with the same structure as the

* Create a new DataFrame with the same structure as the training dataset (X_train).
*  Ensure the number and order of features match exactly.
Fill missing values using the median from the training data.

**‚úÖ Handle Categorical and Numerical Features**

Assign appropriate values for categorical features

* Assign appropriate values for categorical features based on training encoding.

* Ensure all numerical features are correctly formatted (e.g., RAM in GB, weight in kg).

**‚úÖ Standardize Features (if applicable)**

If numerical features were scaled during training

* If numerical features were scaled during training using StandardScaler, apply the same transformation to the new input.
* This ensures the model receives data in the same scale as during training.

**‚úÖ Make Predictions**

Use the trained machine learning model to predict the

* Use the trained machine learning model to predict the laptop price.
* Convert the predicted price back to its original scale (if necessary).

**‚úÖ Display the Prediction**

* Print or return the predicted price in a user-friendly format.
"""

print("Training feature count:", X_train.shape[1])

import pandas as pd
import numpy as np

# Create an empty DataFrame with the same feature names as training data
new_laptop_df = pd.DataFrame(np.zeros((1, len(X_train.columns))), columns=X_train.columns)

# Manually set values for known features
if 'ram' in new_laptop_df.columns:
    new_laptop_df['ram'] = 8                      # 8GB RAM
if 'ssd' in new_laptop_df.columns:
    new_laptop_df['ssd'] = 1                      # 512GB SSD (binary feature)
if 'weight' in new_laptop_df.columns:
    new_laptop_df['weight'] = 2.0                 # 2kg weight

# Fill missing values with the median from training data
for col in new_laptop_df.columns:
    if new_laptop_df[col].isnull().values.any():
        new_laptop_df[col] = X_train[col].median()  # Use training median

# Ensure the DataFrame only contains the exact training columns
new_laptop_df = new_laptop_df[X_train.columns]

# Predict using the trained model
predicted_price = model.predict(new_laptop_df)
print("Predicted Price:", predicted_price[0])

import pandas as pd
import numpy as np

# Example: New laptop input (RAM: 8GB, SSD: 512GB, Weight: 2kg)
new_laptop_data = {'ram': 8, 'ssd': 1, 'weight': 2.0}

# Create an empty DataFrame with the same feature names as training data
new_laptop_df = pd.DataFrame(np.zeros((1, len(X_train.columns))), columns=X_train.columns)

# Assign values to the corresponding features
for feature, value in new_laptop_data.items():
    if feature in new_laptop_df.columns:
        new_laptop_df[feature] = value

# Fill missing values with the median from training data
for col in new_laptop_df.columns:
    if new_laptop_df[col].isnull().values.any():
        new_laptop_df[col] = X_train[col].median()

# Ensure feature order matches training data
new_laptop_df = new_laptop_df[X_train.columns]

# Predict the price using the trained model
predicted_price = model.predict(new_laptop_df)[0]

# Display the predicted price
print(f"Predicted Laptop Price: ${predicted_price:.2f}")

"""**9. Insights & Visualization**

Find key insights into laptop pricing using visualization.
"""

print("Columns in X_train:", X_train.columns)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the processed dataset (Ensure it has the cleaned and feature-engineered data)
df = pd.read_csv("processed_laptop_data.csv")  # Use your cleaned dataset

# Set Seaborn theme
sns.set_style("whitegrid")

# 1Ô∏è Laptop Price Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['price'], bins=30, kde=True, color='blue')
plt.xlabel("Laptop Price (in USD)")
plt.ylabel("Frequency")
plt.title("Distribution of Laptop Prices")
plt.show()

# 2Ô∏è Price vs. RAM
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['ram'], y=df['price'], palette="coolwarm")
plt.xlabel("RAM (GB)")
plt.ylabel("Laptop Price (USD)")
plt.title("Laptop Price vs. RAM")
plt.show()

# 3Ô∏è Price vs. SSD & HDD
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['ssd'], y=df['price'], palette="viridis")
plt.xlabel("SSD Size (GB)")
plt.ylabel("Laptop Price (USD)")
plt.title("Laptop Price vs. SSD Storage")
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['hdd'], y=df['price'], palette="plasma")
plt.xlabel("HDD Size (GB)")
plt.ylabel("Laptop Price (USD)")
plt.title("Laptop Price vs. HDD Storage")
plt.show()

# 4Ô∏è Brand-wise Average Price
plt.figure(figsize=(12, 6))
brand_avg_price = df.groupby('company')['price'].mean().sort_values(ascending=False)
sns.barplot(x=brand_avg_price.index, y=brand_avg_price.values, palette="magma")
plt.xlabel("Laptop Brand")
plt.ylabel("Average Laptop Price (USD)")
plt.xticks(rotation=45)
plt.title("Average Laptop Price by Brand")
plt.show()

# 5Ô∏è OS vs. Price
plt.figure(figsize=(12, 6))
sns.boxplot(x=df['opsys'], y=df['price'], palette="coolwarm")
plt.xlabel("Operating System")
plt.ylabel("Laptop Price (USD)")
plt.xticks(rotation=45)
plt.title("Laptop Price vs. Operating System")
plt.show()

# 6Ô∏è CPU Impact on Price
plt.figure(figsize=(12, 6))
cpu_avg_price = df.groupby('cpu_brand')['price'].mean().sort_values(ascending=False)
sns.barplot(x=cpu_avg_price.index, y=cpu_avg_price.values, palette="rocket")
plt.xlabel("CPU Brand")
plt.ylabel("Average Laptop Price (USD)")
plt.xticks(rotation=45)
plt.title("Average Laptop Price by CPU Brand")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the processed dataset
df = pd.read_csv("/content/laptop.csv")

# Ensure no missing or invalid values
df.replace("?", np.nan, inplace=True)  # Convert "?" to NaN
df.dropna(inplace=True)  # Drop rows with missing values

# Convert all categorical features to numerical (if not already)
for col in df.select_dtypes(include=['object']).columns:
    df[col] = df[col].astype('category').cat.codes  # Label Encoding

# 7Ô∏è Correlation Heatmap (Fixed)
plt.figure(figsize=(12, 8))
corr_matrix = df.corr()  # Compute correlation only for numeric columns
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""** 1)question:**

1.Which features have the most significant impact on laptop prices?

**Answer:**
The most significant features that impact laptop prices are:  

### ** Top Features Influencing Laptop Price:**

1Ô∏è **RAM Size (Higher RAM = Higher Price)**  
   - More RAM improves performance, making the laptop more expensive.  
   - **8GB < 16GB < 32GB < 64GB** in terms of pricing.  

2Ô∏è **Storage Type & Size (SSD > HDD, Larger = Pricier)**  
   - SSDs (Solid State Drives) are **faster and more expensive** than HDDs.  
   - Laptops with **512GB or 1TB SSDs** cost more than **128GB or 256GB SSDs**.  

3Ô∏è **CPU Brand & Model (Intel i9 > i7 > i5 > i3, AMD Ryzen 9 > 7 > 5)**  
   - **Intel Core i9** and **AMD Ryzen 9** are premium CPUs, increasing price.  
   - Gaming and high-performance laptops use **powerful CPUs**, making them more expensive.  

4Ô∏è **Screen Size & Resolution (Larger & Higher Resolution = Higher Price)**  
   - **15.6-inch and 17-inch** screens usually cost more than **13-inch** models.  
   - **4K resolution > 1440p (2K) > 1080p** in terms of price.  

5Ô∏è **GPU (Dedicated GPUs increase price significantly)**  
   - Laptops with **NVIDIA RTX 4060, 4070, 4080, 4090** or **AMD Radeon RX** GPUs are **much more expensive**.  
   - Integrated GPUs (like Intel UHD, Iris Xe) are cheaper but less powerful.  

6Ô∏è **Brand (Apple, Razer, MSI > Dell, HP, Lenovo, Acer)**  
   - Premium brands like **Apple, Razer, MSI** have higher pricing than **Acer, HP, Lenovo** for similar specs.  

7Ô∏è **Weight & Build Quality (Premium = Lighter & More Expensive)**  
   - **Ultrabooks and MacBooks** are lightweight but costly due to premium materials (aluminum, carbon fiber).  
   - Heavier gaming laptops are often priced higher due to better cooling & performance.  

8Ô∏è **Operating System (MacOS > Windows > ChromeOS > Linux)**  
   - **MacBooks (MacOS) are premium** compared to **Windows/Linux laptops**.  
   - **Windows laptops with Pro editions** tend to cost more than Home editions.  

9Ô∏è **Touchscreen & 2-in-1 Convertibles (More Features = Higher Price)**  
   - **Touchscreen and hybrid 2-in-1 laptops (Lenovo Yoga, Surface Pro)** are generally more expensive.  

 **Conclusion:**  
The most influential factors on laptop pricing are **RAM, SSD, CPU, GPU, Screen Size, and Brand**.  
Premium materials, weight, OS, and touchscreens also add to the cost.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1Ô∏è Load Dataset
df = pd.read_csv("/content/laptop.csv")  # Update with actual filename
print("‚úÖ Dataset Loaded")

# 2Ô∏è Data Cleaning & Preprocessing
# Drop unnecessary columns
df = df.drop(columns=['Unnamed: 0'], errors='ignore')

# Convert 'Ram' column to numeric
if 'Ram' in df.columns:
    df['Ram'] = df['Ram'].astype(str).str.replace(r"[^\d]", "", regex=True)  # Remove non-numeric characters
    df['Ram'] = pd.to_numeric(df['Ram'], errors='coerce')  # Convert to numeric, force errors to NaN
    df['Ram'] = df['Ram'].fillna(df['Ram'].median()).astype(int)  # Fill NaN with median
else:
    print("‚ö†Ô∏è Warning: 'Ram' column not found!")

# Convert 'Weight' column to numeric
# Convert 'Weight' column to numeric safely
if 'Weight' in df.columns:
    df['Weight'] = df['Weight'].astype(str).str.replace("kg", "", regex=True)  # Remove "kg"
    df['Weight'] = pd.to_numeric(df['Weight'], errors='coerce')  # Convert to float, turn errors to NaN
    df['Weight'].fillna(df['Weight'].median(), inplace=True)  # Fill NaN with median
else:
    print("‚ö†Ô∏è Warning: 'Weight' column not found!")


# Handle missing values
df.fillna(method='ffill', inplace=True)  # Forward fill missing values

# 3Ô∏è Feature Selection
X = df.drop(columns=['Price'], errors='ignore')  # Features
y = df['Price']  # Target variable

# Identify categorical & numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()

# One-Hot Encoding for categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# 4Ô∏è Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5Ô∏è Model Training (Random Forest Regressor)
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42, n_estimators=100))
])

model.fit(X_train, y_train)
print("‚úÖ Model Trained")

# 6Ô∏è Feature Importance Analysis
feature_names = preprocessor.get_feature_names_out()
feature_importances = model.named_steps['regressor'].feature_importances_

# Create DataFrame for feature importance
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# 7Ô∏è Visualization
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), palette="viridis")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Top 15 Most Important Features for Laptop Price Prediction")
plt.show()

# Print Top 10 Important Features
print("üîπ Top 10 Most Important Features:\n", feature_importance_df.head(10))

"""** 2)question:**

Can the model accurately predict the prices of laptops from lesser-known brands? give me correct answer for this project

**answer:**

The accuracy of the model in predicting laptop prices from lesser-known brands depends on several factors:  

### **1. Data Representation**  
- If the dataset contains sufficient examples of lesser-known brands, the model can learn their pricing patterns.  
- If these brands appear rarely, the model may struggle due to a lack of training data.  

### **2. Feature Influence**  
- **Brand** is just one of many factors affecting price. Features like **CPU, RAM, Storage, and GPU** play a much bigger role in price determination.  
- If brand has a strong impact, the model might overestimate prices for well-known brands and underestimate for lesser-known ones.  

### **3. Model Performance on Lesser-Known Brands**  
- To check this, we can evaluate model errors (e.g., RMSE, MAE) separately for well-known and lesser-known brands.  
- If errors for lesser-known brands are significantly higher, it means the model struggles to predict their prices accurately.  

### **Conclusion**  
 If the dataset has enough data for lesser-known brands, the model can predict prices reasonably well.  
 If there are too few samples, the model may produce unreliable predictions for these brands.  

**Possible Solution:**  
- **Use external data** to improve representation.  
- **Use brand category encoding** (e.g., grouping brands as High-end, Mid-range, Budget).  
- **Train separate models** for different price segments.  

Would you like to analyze model performance on lesser-known brands?
"""

!pip install --upgrade scikit-learn

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Compute metrics
mae = mean_absolute_error(y_test.loc[test_subset.index], subset_predictions)
rmse = np.sqrt(mean_squared_error(y_test.loc[test_subset.index], subset_predictions))  # Manual RMSE
r2 = r2_score(y_test.loc[test_subset.index], subset_predictions)

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")
print(f"R¬≤ Score: {r2}")

"""** 3)question:**

Does the brand of the laptop significantly influence its price?

**Answer:**

Yes, the brand significantly influences laptop prices due to factors like reputation, build quality, performance, and customer service Premium brands (Apple, Dell XPS) are priced higher, while budget brands (Acer, MSI) focus on affordability. Machine learning models (e.g., **Random Forest**) confirm that brand is an important predictor of price. üöÄ
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Drop rows with NaN or fill them (e.g., with median for numerical values)
df.dropna(inplace=True)

# Encode categorical variables
categorical_cols = ['Company', 'TypeName', 'ScreenResolution', 'Cpu', 'Memory', 'Gpu', 'OpSys']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Define features and target
X = df_encoded.drop(columns=['Price'])  # Features
y = df_encoded['Price']  # Target variable

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Get feature importance
importances = model.feature_importances_
feature_names = X.columns

# Display most important features
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)
print(feature_importance_df.head(10))  # Show top 10 important features

"""** 4)question:**

How well does the model perform on laptops with high-end specifications compared to budget laptops?

**Answer:**

The model's performance varies depending on the segment of laptops:  

1. **Budget Laptops (Entry-Level Models)**  
   - These laptops have standard configurations and pricing tends to follow a predictable pattern based on specifications like RAM, storage, and processor type.  
   - Since there is less variability in branding and premium features, the model can accurately predict prices with lower error rates.  

2. **High-End Laptops (Premium & Gaming Models)**  
   - High-end laptops include **premium brands (Apple, Razer, etc.), gaming laptops, and workstations**, which often come with additional price-influencing factors such as build quality, brand reputation, and exclusive features (e.g., Retina displays, custom cooling systems).  
   - Pricing in this segment is more volatile, leading to higher prediction errors as the model struggles to capture brand influence and market demand fluctuations.  

**Overall:**  
The model tends to be more accurate for budget laptops and less reliable for high-end models due to the complex pricing factors beyond technical specifications.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#  Load dataset (Modify the path if needed)
df = pd.read_csv("/content/laptop.csv")

#  Drop unnecessary index columns if present
df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], errors='ignore')

#  Clean column names
df.columns = df.columns.str.strip().str.lower()

#  Remove rows containing "?" in any column
df.replace("?", np.nan, inplace=True)

#  Convert 'Ram' to numeric (removing 'GB')
df['ram'] = df['ram'].astype(str).str.replace("GB", "", regex=True)
df['ram'] = pd.to_numeric(df['ram'], errors='coerce')  # Convert to int, handling errors

#  Convert 'Weight' to numeric (removing 'kg')
df['weight'] = df['weight'].astype(str).str.replace("kg", "", regex=True)
df['weight'] = pd.to_numeric(df['weight'], errors='coerce')  # Convert to float, handling errors

#  Convert 'Price' to numeric
df['price'] = pd.to_numeric(df['price'], errors='coerce')

#  Drop remaining NaN values
df = df.dropna()

#  Define High-End vs. Budget Laptops based on RAM & Price
df['category'] = np.where((df['ram'] >= 16) & (df['price'] > df['price'].median()), 'High-End', 'Budget')

#  Select features for model training
features = ['ram', 'inches', 'weight']
X = df[features]
y = df['price']

#  Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  Train Model (Random Forest Regressor)
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

#  Predictions
y_pred = model.predict(X_test)

#  Performance Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f" Model Performance on Entire Dataset:")
print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤ Score: {r2:.2f}")

#  Performance Comparison for High-End vs. Budget Laptops
high_end = df[df['category'] == 'High-End']
budget = df[df['category'] == 'Budget']

# Get model predictions for each group
high_end_pred = model.predict(high_end[features])
budget_pred = model.predict(budget[features])

# Compute errors separately
high_end_mae = mean_absolute_error(high_end['price'], high_end_pred)
budget_mae = mean_absolute_error(budget['price'], budget_pred)

print("\n Performance by Category:")
print(f" High-End Laptops MAE: {high_end_mae:.2f}")
print(f" Budget Laptops MAE: {budget_mae:.2f}")

#  Visualize Results
plt.figure(figsize=(8,5))
sns.barplot(x=['High-End', 'Budget'], y=[high_end_mae, budget_mae], palette=['blue', 'orange'])
plt.title("Model Performance Comparison (MAE)")
plt.ylabel("Mean Absolute Error (MAE)")
plt.show()

"""** 5)Question:**

What are the limitations and challenges in predicting laptop prices accurately?

**Answer:**

### ** Limitations & Challenges in Predicting Laptop Prices Accurately**  

1. ** Data Quality Issues**  
   - Missing or incorrect values (e.g., `"?"` in weight or RAM).  
   - Inconsistent formatting (e.g., `"16GB"` vs. `"16 GB"`).  

2. ** Feature Complexity**  
   - **Brand Influence:** Lesser-known brands may have unpredictable pricing.  
   - **Component Variability:** CPU, GPU, RAM, and storage significantly impact prices but are not always clearly labeled.  

3. ** Rapid Market Changes**  
   - Frequent price fluctuations due to new releases and promotions.  
   - Discounts, seasonal sales, and regional pricing differences make predictions harder.  

4. **  Model Limitations**  
   - Linear models might struggle to capture complex non-linear price relationships.  
   - Overfitting in models like Random Forest if not tuned properly.  

5. ** Feature Engineering Challenges**  
   - Extracting useful information from unstructured text (e.g., CPU, GPU specifications).  
   - Some specifications have an indirect impact on price, making feature selection tricky.  

6. ** External Factors**  
   - Global supply chain disruptions affect laptop prices.  
   - Brand perception and customer demand are not directly measurable.  

### ** How to Improve Predictions?**
Better Data Cleaning & Feature Engineering

Ensemble Models (e.g., XGBoost, Random Forest) for better accuracy

Regular Data Updates to reflect current pricing trends

** 6)Question:**

How does the model perform when predicting the prices of newly released laptops not present in the training dataset?

**Answer:**

** Model Performance on Newly Released Laptops**

Predicting prices for newly released laptops (not in the training dataset) is challenging due to:

**Lack of Historical Data** ‚Üí No prior pricing trends for these models.

**Feature Mismatch ** **bold text**‚Üí New CPUs, GPUs, and configurations may not be well represented.

**Market Fluctuations** ‚Üí Pricing is affected by demand, supply, and competition.

Testing Model on New Laptops To evaluate performance we can:

Simulate new laptops with unseen specs.

Predict their prices using the trained model.

Compare results with actual market prices (if available).
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#  Load dataset
df = pd.read_csv("/content/laptop.csv")

#  Drop unnecessary columns
df = df.drop(columns=["Unnamed: 0", "Unnamed: 0.1"], errors="ignore")

#  Handle missing values
df.replace("?", np.nan, inplace=True)
df.fillna(method="ffill", inplace=True)  # Forward fill

#  Convert 'Memory' column to numerical format
def convert_memory(memory):
    if "GB" in memory:
        return int(memory.split("GB")[0].strip())  # Extract GB value
    elif "TB" in memory:
        return int(float(memory.split("TB")[0].strip()) * 1000)  # Convert TB to GB
    return 0  # Default case

df["Memory"] = df["Memory"].apply(convert_memory)

#  Convert numerical columns
df["Ram"] = df["Ram"].str.replace("GB", "", regex=True).astype(int)
df["Weight"] = df["Weight"].str.replace("kg", "", regex=True).astype(float)

#  Handle categorical data
categorical_cols = ["Company", "TypeName", "ScreenResolution", "Cpu", "Gpu", "OpSys"]
encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))
encoded_features.columns = encoder.get_feature_names_out(categorical_cols)
df = df.drop(columns=categorical_cols)
df = pd.concat([df, encoded_features], axis=1)

#  Define features and target
X = df.drop(columns=["Price"])
y = df["Price"]

#  Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  Train Model (Random Forest)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

#  Evaluate Model
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f" Model Performance:\nMAE: {mae:.2f}\nRMSE: {rmse:.2f}\nR¬≤ Score: {r2:.2f}")

#  Simulated New Laptop Data (Unseen Specs)
new_laptops = pd.DataFrame({
    "Company": ["Dell", "Apple"],
    "TypeName": ["Ultrabook", "Notebook"],
    "Inches": [14, 13.3],
    "ScreenResolution": ["Full HD 1920x1080", "Retina 2560x1600"],
    "Cpu": ["Intel Core i5 2.5GHz", "Apple M1"],
    "Ram": [8, 16],
    "Memory": ["256GB SSD", "512GB SSD"],
    "Gpu": ["Intel UHD Graphics 620", "Apple M1"],
    "OpSys": ["Windows 10", "MacOS"],
    "Weight": [1.4, 1.3]
})

#  Apply the same 'Memory' transformation
new_laptops["Memory"] = new_laptops["Memory"].apply(convert_memory)

#  Encode New Laptop Data
encoded_new_features = pd.DataFrame(encoder.transform(new_laptops[categorical_cols]))
encoded_new_features.columns = encoder.get_feature_names_out(categorical_cols)
new_laptops = new_laptops.drop(columns=categorical_cols)
new_laptops = pd.concat([new_laptops, encoded_new_features], axis=1)

#  Ensure new_laptops has the same features as X_train
new_laptops = new_laptops.reindex(columns=X_train.columns, fill_value=0)

#  Predict Prices for New Laptops
predicted_prices = model.predict(new_laptops)

#  Display Results
new_laptops["Predicted Price"] = predicted_prices
print(new_laptops[["Predicted Price"]])